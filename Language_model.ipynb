{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Language_model.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1dOncC2N9gAR7xOiAzQh27fZ5ft56XYdm","authorship_tag":"ABX9TyMl6SBv6wBtoEo3AgwdZ1du"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"YLJ8mAVVo4Ye","colab_type":"code","colab":{}},"source":["# Author: Arman Kabiri\n","# Date: Feb. 18, 2020\n","# Email: Arman.Kabiri94@gmail.com"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R4GU9bOKlZgy","colab_type":"code","outputId":"18932a56-6a29-4c75-df25-55c8310f428f","executionInfo":{"status":"ok","timestamp":1584124859768,"user_tz":180,"elapsed":22113,"user":{"displayName":"Arman Kabiri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhEhmp3MUuNcUVj9CDTifSEk2lm_aME6fhPGB-=s64","userId":"08040985308833979547"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uK1Na68GmLMb","colab_type":"code","outputId":"6fe88bff-0ef3-44b7-8d72-88fe5a8b8ec0","executionInfo":{"status":"ok","timestamp":1584124864090,"user_tz":180,"elapsed":2225,"user":{"displayName":"Arman Kabiri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhEhmp3MUuNcUVj9CDTifSEk2lm_aME6fhPGB-=s64","userId":"08040985308833979547"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import os\n","os.chdir('/gdrive/My Drive/NLP_Stuff/My_Language_Model')\n","!pwd"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/gdrive/My Drive/NLP_Stuff/My_Language_Model\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IZ_wEUt9iBfV","colab_type":"code","colab":{}},"source":["import argparse\n","import math\n","import os.path as path\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","\n","from CorpusReader import CorpusReader\n","from Dictionary import Dictionary\n","from EmbeddingsLoader import EmbeddingsLoader\n","from Lang_Model import LanguageModel"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nsk3Vi1xf8i9","colab_type":"code","colab":{}},"source":["class Args:\n","  corpus_train_file='Data/corpus-test.txt'\n","  corpus_valid_file=''\n","  embeddings_file='Data/English_Wiki_1Billion_embeddings.bin'\n","  output_model_path='Data/model.bin'\n","  n_layers=2\n","  hidden_size=300\n","  dropout_probablity=.25\n","  embeddings_dim=300\n","  batch_size=50\n","  seq_len=20\n","  epochs=2\n","  lr=0.001\n","  seed=120\n","  bidirectional_model=False\n","  tie_weights=False\n","  freez_embeddings=False\n","  gpu=True\n","  \n","args = Args()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PYPZK2k9qLEA","colab_type":"code","outputId":"9e5c195e-8c1d-4e30-b13e-f5c18a705e5c","executionInfo":{"status":"ok","timestamp":1584104806817,"user_tz":180,"elapsed":29620,"user":{"displayName":"Arman Kabiri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhEhmp3MUuNcUVj9CDTifSEk2lm_aME6fhPGB-=s64","userId":"08040985308833979547"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["torch.cuda.is_available()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"bT7ytKqnj2_J","colab_type":"code","outputId":"f47a53c4-06e9-4ad9-f607-2c30235c3463","executionInfo":{"status":"ok","timestamp":1584104913038,"user_tz":180,"elapsed":135831,"user":{"displayName":"Arman Kabiri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhEhmp3MUuNcUVj9CDTifSEk2lm_aME6fhPGB-=s64","userId":"08040985308833979547"}},"colab":{"base_uri":"https://localhost:8080/","height":245}},"source":["def main():\n","    torch.set_num_threads(8)\n","\n","    if torch.cuda.is_available():\n","        if not args.gpu:\n","            print(\"WARNING: You have a CUDA device, so you should probably run with --gpu\")\n","    else:\n","        if args.gpu:\n","            print(\"You do not have a GPU device, so you should run CPU without --gpu option.\")\n","            exit()\n","\n","    torch.manual_seed(args.seed)\n","    corpus_train_reader = CorpusReader(args.corpus_train_file, 10000000)  # 100MB\n","\n","    print(\"Generating Dictionaries\")\n","    dictionary = Dictionary(corpus_train_reader)\n","    dictionary.build_dictionary()\n","\n","    print(\"Loading Embeddings\")\n","\n","    embeddings_matrix = None\n","    if args.embeddings_file is not None:\n","        emb_loader = EmbeddingsLoader()\n","        embeddings_matrix = emb_loader.get_embeddings_matrix(args.embeddings_file, dictionary, args.embeddings_dim)\n","\n","    model = LanguageModel(n_layers=args.n_layers, hidden_size=args.hidden_size, n_vocab=dictionary.get_dic_size(),\n","                          input_size=args.embeddings_dim, dropout=args.dropout_probablity,\n","                          bidirectional=args.bidirectional_model, pret_emb_matrix=embeddings_matrix,\n","                          freez_emb=args.freez_embeddings, tie_weights=args.tie_weights, use_gpu=args.gpu)\n","\n","    ###############\n","    total_param = []\n","    for p in model.parameters():\n","        total_param.append(int(p.numel()))\n","    print(total_param)\n","    print(sum(total_param))\n","    ###############\n","\n","    if path.exists(args.output_model_path):\n","        model.load_state_dict(torch.load(args.output_model_path))\n","\n","    else:\n","        # put it into train mode.\n","        model.train()\n","        if args.gpu:\n","            model.cuda()\n","\n","        # Optimizer and Loss\n","        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n","        criterion = nn.CrossEntropyLoss()\n","\n","        print(\"Training starts ...\")\n","        for i in range(args.epochs):\n","            print(f\"Epoch {i + 1}:\")\n","            train(corpus_train_reader, dictionary, model, optimizer, criterion, args)\n","\n","        print(\"Saving Model...\")\n","        torch.save(model.state_dict(), args.output_model_path)\n","\n","    # Text Generation:\n","    generate_text(model, dictionary, 'Cat'.lower(), 7)\n","\n","\n","def train(corpus_train_reader, dictionary, model, optimizer, criterion, args):\n","    batch_generator = corpus_train_reader.batchify(dictionary, args.batch_size, args.seq_len)\n","    hidden = model.init_hidden(args.batch_size)\n","\n","    step = 0\n","    for x, y in tqdm(batch_generator):\n","\n","        step += 1\n","        x = torch.from_numpy(x)\n","        y = torch.from_numpy(y)\n","\n","        if args.gpu:\n","            x = x.cuda()\n","            y = y.cuda()\n","\n","        hidden = detach_hidden(hidden)\n","        model.zero_grad()\n","\n","        y_hat, hidden = model.forward(x, hidden)\n","\n","        loss = criterion.forward(y_hat.view(-1, dictionary.get_dic_size()),\n","                                 y.reshape(args.batch_size * args.seq_len).long())\n","        loss.backward()\n","\n","        # TODO: POSSIBLE EXPLODING GRADIENT PROBLEM! -> CLIP JUST IN CASE :\n","        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n","\n","        optimizer.step()\n","\n","        if step % 100 == 0:\n","            print(f\"Step {step},     Loss = {loss.item()},    PPL = {math.exp(loss)}\")\n","\n","\n","def detach_hidden(hidden: tuple):\n","\n","    return tuple(v.detach() for v in hidden)\n","\n","\n","def generate_text(model: LanguageModel, dictionary: Dictionary, seed: str, k=1):\n","\n","    if args.gpu:\n","        model.cuda()\n","    else:\n","        model.cpu()\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","        hidden = model.init_hidden(1)\n","        input_text = seed\n","        output = [seed]\n","\n","        for i in range(10):\n","            word, hidden = predict_next_word(model, dictionary, hidden, input_text, k)\n","            output.append(word)\n","            input_text = word\n","\n","    print(' '.join(output))\n","\n","\n","def predict_next_word(model: LanguageModel, dictionary, hidden, input_text: str, k=1) -> tuple:\n","    input_tensor = dictionary.encode_text(input_text)\n","    input_tensor = np.array(input_tensor)\n","    input_tensor = torch.from_numpy(input_tensor)\n","    if args.gpu:\n","        input_tensor = input_tensor.cuda()\n","\n","    input_tensor = input_tensor.view(-1,1)\n","    output, hidden = model.forward(input_tensor, hidden)\n","    # TODO here\n","    probs = F.softmax(output, 2)\n","\n","    # move back to CPU to use with numpy\n","    if args.gpu:\n","        probs = probs.cpu()\n","\n","    probs, picked_indexes = probs.topk(k)\n","    picked_indexes = picked_indexes.numpy().squeeze()\n","    probs = probs.numpy().flatten()\n","    probs = probs / probs.sum()\n","    word = np.random.choice(picked_indexes, p=probs)\n","\n","    word = dictionary.decode_text([word.item()])\n","\n","    return word, hidden\n","\n","\n","\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\r0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Generating Dictionaries\n","Building dictionaries...\n"],"name":"stdout"},{"output_type":"stream","text":["11it [00:04,  2.23it/s]\n","/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"stream","text":["Dictionaries are built - Vocab size is 254732\n","Loading Embeddings\n","Loading pretrained embeddings...\n","Pretrained embeddings are loaded.\n","[76419600, 360000, 360000, 1200, 1200, 360000, 360000, 1200, 1200, 76419600, 254732]\n","154538732\n","cat in the book is the most widely used example of\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UiNBqwsOqy-m","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}